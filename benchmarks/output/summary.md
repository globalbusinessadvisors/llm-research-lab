# LLM Research Lab Benchmark Results

Canonical benchmark results for LLM Research Lab operations.

**Generated:** Initial template - run benchmarks to populate results

**Total Benchmarks:** 0 (run `llm-research run` to execute benchmarks)

## Summary

| Target | Duration (ms) | Status | Timestamp |
|--------|---------------|--------|-----------|
| _No results yet_ | - | - | - |

## Available Benchmark Targets

The following benchmark targets are registered and available for execution:

### Metrics Category
- `accuracy-metric` - Benchmarks accuracy metric computation across comparison modes
- `bleu-metric` - Benchmarks BLEU score computation with n-gram analysis
- `rouge-metric` - Benchmarks ROUGE-L metric computation
- `latency-metric` - Benchmarks latency metric computation including TTFT
- `perplexity-metric` - Benchmarks perplexity computation from log probabilities
- `metric-aggregator` - Benchmarks metric aggregation (mean, median, percentiles)
- `statistical-analysis` - Benchmarks statistical analysis (t-tests, effect sizes)

### Evaluators Category
- `batch-evaluation` - Benchmarks batch evaluation of prediction-reference pairs
- `comparative-evaluation` - Benchmarks comparative evaluation across multiple models
- `llm-judge-evaluation` - Benchmarks LLM-as-Judge evaluation pattern

### Workflows Category
- `pipeline-orchestration` - Benchmarks DAG pipeline orchestration with dependencies
- `task-execution` - Benchmarks task execution framework overhead
- `data-loading` - Benchmarks dataset loading and preprocessing

### Scoring Category
- `heuristic-scoring` - Benchmarks heuristic scoring algorithms
- `model-ranking` - Benchmarks model ranking and selection strategies
- `chain-of-thought` - Benchmarks chain-of-thought reasoning evaluation

## Usage

Run all benchmarks:
```bash
llm-research run
```

Run specific benchmarks:
```bash
llm-research run --targets accuracy-metric,bleu-metric
```

List available targets:
```bash
llm-research run --list
```

## Output Files

Results are stored in:
- `benchmarks/output/` - Combined benchmark result files
- `benchmarks/output/raw/` - Individual benchmark result files (JSON)
- `benchmarks/output/summary.md` - This summary file (auto-updated)

---

_Generated by LLM Research Lab Canonical Benchmark System_
