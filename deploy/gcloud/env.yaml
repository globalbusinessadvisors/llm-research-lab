# =============================================================================
# LLM-Research-Lab Environment Configuration
# =============================================================================
#
# This file defines ALL required environment variables for LLM-Research-Lab.
#
# CONSTITUTION COMPLIANCE:
#   - No agent hardcodes service names or URLs
#   - No agent embeds credentials, secrets, or mutable state
#   - All dependencies resolve via environment variables or Secret Manager
#   - LLM-Research-Lab does NOT connect directly to Google SQL
#   - ALL persistence occurs via ruvector-service
#
# USAGE:
#   Copy this template and fill in values for your environment.
#   Deploy using: gcloud run services update --env-vars-file=env.yaml
# =============================================================================

# =============================================================================
# SERVICE IDENTIFICATION (REQUIRED)
# =============================================================================

# Service name - must match Cloud Run service name
SERVICE_NAME: llm-research-lab

# Service version - typically set by CI/CD from git SHA
SERVICE_VERSION: "1.0.0"

# Platform environment: dev | staging | prod
PLATFORM_ENV: dev

# =============================================================================
# RUVECTOR SERVICE CONFIGURATION (REQUIRED)
# =============================================================================
#
# LLM-Research-Lab does NOT own persistence.
# ALL data is persisted via ruvector-service.
# LLM-Research-Lab NEVER connects directly to Google SQL.
#

# RuVector service URL (internal VPC endpoint preferred)
# Format: https://ruvector-service-{env}.run.app or internal URL
RUVECTOR_SERVICE_URL: https://ruvector-service-dev.run.app

# RuVector authentication token
# MUST be stored in Secret Manager, not here
# Reference: projects/agentics-dev/secrets/ruvector-auth-token/versions/latest
# RUVECTOR_AUTH_TOKEN: (from Secret Manager)

# Request timeout for ruvector-service calls (milliseconds)
RUVECTOR_TIMEOUT_SECS: "30"

# Maximum retry attempts for ruvector-service
RUVECTOR_MAX_RETRIES: "3"

# =============================================================================
# TELEMETRY & OBSERVABILITY (REQUIRED)
# =============================================================================
#
# Telemetry compatible with LLM-Observatory
#

# LLM-Observatory endpoint for telemetry
LLM_OBSERVATORY_ENDPOINT: https://llm-observatory-dev.run.app

# Telemetry endpoint (may be same as Observatory)
TELEMETRY_ENDPOINT: https://llm-observatory-dev.run.app/api/v1/telemetry

# OpenTelemetry collector endpoint (if using OTLP export)
OTEL_EXPORTER_OTLP_ENDPOINT: https://otel-collector-dev.run.app

# Enable telemetry to stdout (for Cloud Logging)
TELEMETRY_STDOUT: "true"

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================

# Rust log level filter
# Format: level or module=level
RUST_LOG: info,llm_research_agents=debug,llm_research_api=debug

# Application log level
LLM_RESEARCH_LOG_LEVEL: info

# =============================================================================
# APPLICATION CONFIGURATION
# =============================================================================

# HTTP server port
LLM_RESEARCH_PORT: "8080"

# HTTP server host
LLM_RESEARCH_HOST: "0.0.0.0"

# =============================================================================
# AGENT CONFIGURATION
# =============================================================================

# Minimum sample size for hypothesis evaluation
HYPOTHESIS_MIN_SAMPLE_SIZE: "30"

# Default significance level (alpha)
HYPOTHESIS_DEFAULT_ALPHA: "0.05"

# Minimum sample size for metrics computation
METRIC_MIN_SAMPLE_SIZE: "3"

# Default decimal precision
METRIC_DEFAULT_PRECISION: "4"

# =============================================================================
# INTEGRATION ENDPOINTS (OPTIONAL)
# =============================================================================
#
# These are consumers that MAY receive Research-Lab outputs.
# Research-Lab does NOT invoke these directly - they consume DecisionEvents.
#

# LLM-CostOps endpoint (consumer, read-only reference)
# LLM_COSTOPS_ENDPOINT: https://llm-costops-dev.run.app

# LLM-Latency-Lens endpoint (consumer, read-only reference)
# LLM_LATENCY_LENS_ENDPOINT: https://llm-latency-lens-dev.run.app

# Governance audit endpoint (consumer)
# GOVERNANCE_AUDIT_ENDPOINT: https://governance-audit-dev.run.app
